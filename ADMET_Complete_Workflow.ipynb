{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMET Prediction Models - Complete Workflow\n",
    "\n",
    "This notebook demonstrates a complete end-to-end workflow for ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) property prediction using machine learning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "ADMET properties are critical pharmacokinetic and safety parameters evaluated during drug discovery. This workflow includes:\n",
    "\n",
    "1. **Data Loading and Preprocessing**: Loading molecular data and calculating features\n",
    "2. **Data Splitting**: Stratified and scaffold-based splitting strategies\n",
    "3. **Making Predictions**: Using pre-trained models for all 11 ADMET properties\n",
    "4. **Results Visualization**: Analyzing and visualizing prediction results\n",
    "5. **Custom Model Training**: Training your own ADMET models (optional)\n",
    "\n",
    "## Available ADMET Models\n",
    "\n",
    "| Model Name | Property | Classification Criteria |\n",
    "|------------|----------|-------------------------|\n",
    "| BBB | Blood-Brain Barrier | logBB ≥ -1: permeable |\n",
    "| Papp | Caco-2 Permeability | Papp ≥ 8×10⁻⁶ cm/s: permeable |\n",
    "| P_gp_subs | P-glycoprotein Substrate | ER ≥ 2: substrate |\n",
    "| CYP1A2 | CYP1A2 Inhibition | IC50 < 10 µM: inhibitor |\n",
    "| CYP2C9 | CYP2C9 Inhibition | IC50 < 10 µM: inhibitor |\n",
    "| CYP2C19 | CYP2C19 Inhibition | IC50 < 10 µM: inhibitor |\n",
    "| CYP2D6 | CYP2D6 Inhibition | IC50 < 10 µM: inhibitor |\n",
    "| CYP3A4 | CYP3A4 Inhibition | IC50 < 10 µM: inhibitor |\n",
    "| HCLint | Human Hepatic Clearance | t½ > 30 min: stable |\n",
    "| RCLint | Rat Hepatic Clearance | t½ > 30 min: stable |\n",
    "| hERG_inh | hERG Inhibition | IC50 < 10 µM: inhibitor |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# RDKit for molecular handling\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom modules\n",
    "from utils import data_preprocessing, load_features\n",
    "from predict import load_model, predict\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Let's load a sample dataset and preprocess it. We'll use the included `smiles.csv` file as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample data\n",
    "data_file = 'smiles.csv'\n",
    "print(f\"Loading data from: {data_file}\")\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "print(f\"\\nLoaded {len(df)} molecules\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "class_counts = df['bioclass'].value_counts()\n",
    "print(class_counts)\n",
    "print(f\"\\nClass 0: {class_counts[0]} ({class_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Class 1: {class_counts[1]} ({class_counts[1]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts.plot(kind='bar', color=['#FF6B6B', '#4ECDC4'])\n",
    "plt.title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some example molecules\n",
    "print(\"Sample molecules from the dataset:\\n\")\n",
    "\n",
    "# Take 4 random molecules\n",
    "sample_smiles = df.sample(4, random_state=42)['SMILES'].tolist()\n",
    "mols = [Chem.MolFromSmiles(smi) for smi in sample_smiles]\n",
    "\n",
    "# Display molecules\n",
    "img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(300, 300), \n",
    "                            legends=[f\"Molecule {i+1}\" for i in range(len(mols))])\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making Predictions with Pre-trained Models\n",
    "\n",
    "Now let's use all 11 pre-trained ADMET models to make predictions on our molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all available ADMET models\n",
    "admet_models = [\n",
    "    'BBB',       # Blood-Brain Barrier\n",
    "    'Papp',      # Caco-2 Permeability\n",
    "    'P_gp_subs', # P-glycoprotein Substrate\n",
    "    'CYP1A2',    # CYP1A2 Inhibition\n",
    "    'CYP2C9',    # CYP2C9 Inhibition\n",
    "    'CYP2C19',   # CYP2C19 Inhibition\n",
    "    'CYP2D6',    # CYP2D6 Inhibition\n",
    "    'CYP3A4',    # CYP3A4 Inhibition\n",
    "    'HCLint',    # Human Hepatic Clearance\n",
    "    'RCLint',    # Rat Hepatic Clearance\n",
    "    'hERG_inh'   # hERG Inhibition\n",
    "]\n",
    "\n",
    "print(f\"Available ADMET models: {len(admet_models)}\")\n",
    "for i, model in enumerate(admet_models, 1):\n",
    "    print(f\"{i:2d}. {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features for preprocessing\n",
    "features = load_features()\n",
    "print(f\"Loaded {len(features)} molecular features for model input\")\n",
    "\n",
    "# Preprocess the data once (this is used by all models)\n",
    "print(\"\\nPreprocessing data...\")\n",
    "scaled_data = data_preprocessing(df.copy())\n",
    "print(f\"✓ Preprocessing complete. Final dataset: {len(scaled_data)} molecules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with all models\n",
    "print(\"Making predictions with all ADMET models...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store all predictions\n",
    "all_predictions = pd.DataFrame({'SMILES': scaled_data['SMILES']})\n",
    "\n",
    "# Dictionary to store probability predictions\n",
    "all_probabilities = {}\n",
    "\n",
    "for model_name in admet_models:\n",
    "    try:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model(model_name)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(scaled_data[features].values)\n",
    "        probabilities = model.predict_proba(scaled_data[features].values)\n",
    "        max_probs = np.max(probabilities, axis=1)\n",
    "        \n",
    "        # Store predictions\n",
    "        all_predictions[model_name] = predictions\n",
    "        all_predictions[f\"{model_name}_prob\"] = max_probs\n",
    "        all_probabilities[model_name] = probabilities\n",
    "        \n",
    "        # Calculate statistics\n",
    "        positive_count = np.sum(predictions == 1)\n",
    "        negative_count = np.sum(predictions == 0)\n",
    "        positive_pct = (positive_count / len(predictions)) * 100\n",
    "        \n",
    "        print(f\"  ✓ Predictions complete\")\n",
    "        print(f\"  Positive (1): {positive_count:3d} ({positive_pct:5.1f}%)\")\n",
    "        print(f\"  Negative (0): {negative_count:3d} ({100-positive_pct:5.1f}%)\")\n",
    "        print(f\"  Avg confidence: {np.mean(max_probs):.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ All predictions complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display prediction results\n",
    "print(\"\\nPrediction Results Summary:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show predictions for first 10 molecules (only binary predictions)\n",
    "display_cols = ['SMILES'] + [m for m in admet_models if m in all_predictions.columns]\n",
    "print(\"\\nFirst 10 molecules (Binary predictions: 1=positive, 0=negative):\")\n",
    "display(all_predictions[display_cols].head(10))\n",
    "\n",
    "# Save complete predictions to CSV\n",
    "output_file = 'all_admet_predictions.csv'\n",
    "all_predictions.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Complete predictions saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Visualization\n",
    "\n",
    "Let's visualize the prediction results across all ADMET properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distribution across all models\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(admet_models):\n",
    "    if model_name in all_predictions.columns:\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Count predictions\n",
    "        counts = all_predictions[model_name].value_counts()\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(['Negative (0)', 'Positive (1)'], \n",
    "                      [counts.get(0, 0), counts.get(1, 0)],\n",
    "                      color=['#FF6B6B', '#4ECDC4'])\n",
    "        \n",
    "        ax.set_title(f'{model_name}', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Count', fontsize=10)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        total = len(all_predictions)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height/total*100:.1f}%',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.suptitle('ADMET Prediction Distribution Across All Models', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap showing prediction patterns\n",
    "prediction_matrix = all_predictions[[m for m in admet_models if m in all_predictions.columns]].T\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(prediction_matrix.iloc[:, :50],  # Show first 50 molecules\n",
    "            cmap=['#FF6B6B', '#4ECDC4'], \n",
    "            cbar_kws={'label': 'Prediction (0=Negative, 1=Positive)'},\n",
    "            linewidths=0.5,\n",
    "            linecolor='white',\n",
    "            vmin=0, vmax=1)\n",
    "\n",
    "plt.title('ADMET Prediction Heatmap (First 50 Molecules)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Molecule Index', fontsize=12)\n",
    "plt.ylabel('ADMET Property', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence across models\n",
    "prob_cols = [f\"{m}_prob\" for m in admet_models if f\"{m}_prob\" in all_predictions.columns]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Box plot of prediction probabilities\n",
    "prob_data = [all_predictions[col].values for col in prob_cols]\n",
    "model_labels = [col.replace('_prob', '') for col in prob_cols]\n",
    "\n",
    "bp = plt.boxplot(prob_data, labels=model_labels, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#4ECDC4')\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.title('Prediction Confidence Distribution Across ADMET Models', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Prediction Probability', fontsize=12)\n",
    "plt.xlabel('ADMET Model', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary_stats = []\n",
    "\n",
    "for model_name in admet_models:\n",
    "    if model_name in all_predictions.columns:\n",
    "        preds = all_predictions[model_name]\n",
    "        probs = all_predictions[f\"{model_name}_prob\"]\n",
    "        \n",
    "        summary_stats.append({\n",
    "            'Model': model_name,\n",
    "            'Positive (%)': f\"{np.sum(preds == 1) / len(preds) * 100:.1f}%\",\n",
    "            'Negative (%)': f\"{np.sum(preds == 0) / len(preds) * 100:.1f}%\",\n",
    "            'Mean Confidence': f\"{np.mean(probs):.3f}\",\n",
    "            'Median Confidence': f\"{np.median(probs):.3f}\",\n",
    "            'Min Confidence': f\"{np.min(probs):.3f}\",\n",
    "            'Max Confidence': f\"{np.max(probs):.3f}\"\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "print(\"\\nSummary Statistics for All ADMET Models:\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Analysis for Individual Molecules\n",
    "\n",
    "Let's examine the ADMET profile for specific molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific molecule\n",
    "molecule_idx = 0  # Change this to examine different molecules\n",
    "\n",
    "print(f\"\\nADMET Profile for Molecule #{molecule_idx}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get SMILES and draw molecule\n",
    "smiles = all_predictions.iloc[molecule_idx]['SMILES']\n",
    "print(f\"SMILES: {smiles}\\n\")\n",
    "\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "display(Draw.MolToImage(mol, size=(400, 400)))\n",
    "\n",
    "# Show predictions\n",
    "print(\"\\nADMET Predictions:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name in admet_models:\n",
    "    if model_name in all_predictions.columns:\n",
    "        pred = all_predictions.iloc[molecule_idx][model_name]\n",
    "        prob = all_predictions.iloc[molecule_idx][f\"{model_name}_prob\"]\n",
    "        \n",
    "        result = \"✓ Positive\" if pred == 1 else \"✗ Negative\"\n",
    "        print(f\"{model_name:12s}: {result:12s} (confidence: {prob:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for ADMET profile of a molecule\n",
    "from math import pi\n",
    "\n",
    "# Get predictions for the molecule\n",
    "molecule_predictions = []\n",
    "categories = []\n",
    "\n",
    "for model_name in admet_models:\n",
    "    if model_name in all_predictions.columns:\n",
    "        pred = all_predictions.iloc[molecule_idx][model_name]\n",
    "        molecule_predictions.append(pred)\n",
    "        categories.append(model_name)\n",
    "\n",
    "# Number of variables\n",
    "N = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "molecule_predictions += molecule_predictions[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Draw the plot\n",
    "ax.plot(angles, molecule_predictions, 'o-', linewidth=2, color='#4ECDC4', label='Predictions')\n",
    "ax.fill(angles, molecule_predictions, alpha=0.25, color='#4ECDC4')\n",
    "\n",
    "# Fix axis to go in the right order\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=10)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0, 0.5, 1])\n",
    "ax.set_yticklabels(['0', '0.5', '1'])\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "plt.title(f'ADMET Profile - Molecule #{molecule_idx}\\n{smiles[:50]}...', \n",
    "          size=12, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting for Model Training (Optional)\n",
    "\n",
    "If you want to train custom models, you'll need to split your data. Here's how to do it using different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Data Splitting Examples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Stratified split (maintains class distribution)\n",
    "train_data, test_data = train_test_split(\n",
    "    scaled_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=scaled_data['bioclass']\n",
    ")\n",
    "\n",
    "print(\"\\nStratified Split:\")\n",
    "print(f\"  Training set: {len(train_data)} molecules\")\n",
    "print(f\"  Test set:     {len(test_data)} molecules\")\n",
    "print(f\"\\n  Training set class distribution:\")\n",
    "print(f\"    Class 0: {np.sum(train_data['bioclass'] == 0)} ({np.sum(train_data['bioclass'] == 0)/len(train_data)*100:.1f}%)\")\n",
    "print(f\"    Class 1: {np.sum(train_data['bioclass'] == 1)} ({np.sum(train_data['bioclass'] == 1)/len(train_data)*100:.1f}%)\")\n",
    "print(f\"\\n  Test set class distribution:\")\n",
    "print(f\"    Class 0: {np.sum(test_data['bioclass'] == 0)} ({np.sum(test_data['bioclass'] == 0)/len(test_data)*100:.1f}%)\")\n",
    "print(f\"    Class 1: {np.sum(test_data['bioclass'] == 1)} ({np.sum(test_data['bioclass'] == 1)/len(test_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scaffold-based split (more realistic for drug discovery)\n",
    "from dgllife.utils import ScaffoldSplitter\n",
    "\n",
    "# Prepare data for scaffold splitter\n",
    "scaffold_df = scaled_data.copy()\n",
    "scaffold_df = scaffold_df.rename(columns={'SMILES': 'smiles'})\n",
    "\n",
    "# Split by scaffolds\n",
    "train_set, val_set, test_set = ScaffoldSplitter.train_val_test_split(\n",
    "    scaffold_df,\n",
    "    frac_train=0.8,\n",
    "    frac_val=0.0,\n",
    "    frac_test=0.2\n",
    ")\n",
    "\n",
    "scaffold_train = scaffold_df.iloc[train_set.indices]\n",
    "scaffold_test = scaffold_df.iloc[test_set.indices]\n",
    "\n",
    "print(\"\\nScaffold-Based Split:\")\n",
    "print(f\"  Training set: {len(scaffold_train)} molecules\")\n",
    "print(f\"  Test set:     {len(scaffold_test)} molecules\")\n",
    "print(f\"\\n  Training set class distribution:\")\n",
    "print(f\"    Class 0: {np.sum(scaffold_train['bioclass'] == 0)} ({np.sum(scaffold_train['bioclass'] == 0)/len(scaffold_train)*100:.1f}%)\")\n",
    "print(f\"    Class 1: {np.sum(scaffold_train['bioclass'] == 1)} ({np.sum(scaffold_train['bioclass'] == 1)/len(scaffold_train)*100:.1f}%)\")\n",
    "print(f\"\\n  Test set class distribution:\")\n",
    "print(f\"    Class 0: {np.sum(scaffold_test['bioclass'] == 0)} ({np.sum(scaffold_test['bioclass'] == 0)/len(scaffold_test)*100:.1f}%)\")\n",
    "print(f\"    Class 1: {np.sum(scaffold_test['bioclass'] == 1)} ({np.sum(scaffold_test['bioclass'] == 1)/len(scaffold_test)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ Scaffold split prevents data leakage by grouping similar molecules together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training (Optional - Advanced)\n",
    "\n",
    "To train a custom model, you would use the `model.py` script. Here's the workflow:\n",
    "\n",
    "### Step 1: Prepare your data\n",
    "- CSV file with 'SMILES' and 'bioclass' columns\n",
    "\n",
    "### Step 2: Split the data\n",
    "```python\n",
    "# Save split data\n",
    "train_data.to_csv('train_mydata.csv', index=False)\n",
    "test_data.to_csv('test_mydata.csv', index=False)\n",
    "```\n",
    "\n",
    "### Step 3: Train the model (run in terminal)\n",
    "```bash\n",
    "python model.py --file_name mydata.csv --model_name my_custom_model --max_eval 200 --time_out 120 --training\n",
    "```\n",
    "\n",
    "### Model Training Parameters:\n",
    "- `--max_eval`: Number of hyperparameter optimization trials (higher = better but slower)\n",
    "- `--time_out`: Maximum seconds per trial\n",
    "- The training uses Hyperopt with TPE (Tree-structured Parzen Estimator) for optimization\n",
    "- Automatically tries different classifiers and preprocessing methods\n",
    "\n",
    "### Performance Metrics:\n",
    "After training, you'll see:\n",
    "- **Sensitivity (Recall)**: True positive rate\n",
    "- **Specificity**: True negative rate\n",
    "- **Accuracy**: Overall accuracy\n",
    "- **MCC**: Matthews Correlation Coefficient (good for imbalanced data)\n",
    "- **AUC-ROC**: Area under ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save training and test sets for custom model training\n",
    "# Uncomment the lines below to save your split data\n",
    "\n",
    "# train_data.to_csv('train_custom_model.csv', index=False)\n",
    "# test_data.to_csv('test_custom_model.csv', index=False)\n",
    "# print(\"✓ Training and test sets saved for custom model training\")\n",
    "# print(\"\\nTo train a model, run in terminal:\")\n",
    "# print(\"python model.py --file_name custom_model.csv --model_name my_model --max_eval 200 --training\")\n",
    "\n",
    "print(\"Custom model training is available via the command line.\")\n",
    "print(\"See the markdown cell above for instructions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exporting Results\n",
    "\n",
    "Let's export our predictions in various formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed results\n",
    "print(\"Exporting results...\\n\")\n",
    "\n",
    "# 1. Complete predictions with all probabilities\n",
    "all_predictions.to_csv('all_admet_predictions_detailed.csv', index=False)\n",
    "print(\"✓ Detailed predictions saved to: all_admet_predictions_detailed.csv\")\n",
    "\n",
    "# 2. Summary statistics\n",
    "summary_df.to_csv('admet_summary_statistics.csv', index=False)\n",
    "print(\"✓ Summary statistics saved to: admet_summary_statistics.csv\")\n",
    "\n",
    "# 3. Binary predictions only (for easier viewing)\n",
    "binary_cols = ['SMILES'] + [m for m in admet_models if m in all_predictions.columns]\n",
    "all_predictions[binary_cols].to_csv('all_admet_predictions_binary.csv', index=False)\n",
    "print(\"✓ Binary predictions saved to: all_admet_predictions_binary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Workflow complete! All results have been saved.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete ADMET prediction workflow:\n",
    "\n",
    "### ✓ Completed Steps:\n",
    "1. **Data Loading**: Loaded and explored molecular data\n",
    "2. **Preprocessing**: Calculated molecular descriptors and fingerprints\n",
    "3. **Predictions**: Used all 11 pre-trained ADMET models\n",
    "4. **Visualization**: Created comprehensive visualizations of results\n",
    "5. **Analysis**: Examined individual molecule ADMET profiles\n",
    "6. **Data Splitting**: Demonstrated stratified and scaffold-based splitting\n",
    "7. **Export**: Saved results in multiple formats\n",
    "\n",
    "### Next Steps:\n",
    "- Use the predictions to prioritize compounds for synthesis/testing\n",
    "- Train custom models on your own data using the splitting methods shown\n",
    "- Integrate predictions into your drug discovery pipeline\n",
    "- Perform cross-validation to assess model robustness\n",
    "\n",
    "### Key Files Generated:\n",
    "- `all_admet_predictions.csv` - Complete predictions with probabilities\n",
    "- `all_admet_predictions_detailed.csv` - Detailed predictions\n",
    "- `all_admet_predictions_binary.csv` - Binary predictions only\n",
    "- `admet_summary_statistics.csv` - Summary statistics\n",
    "\n",
    "### For More Information:\n",
    "- See `README.md` for detailed documentation\n",
    "- Check `predict.py`, `model.py`, and `utils.py` for implementation details\n",
    "- Refer to the classification criteria table at the top of this notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This workflow uses pre-trained models optimized on specific datasets. For production use, validate predictions with experimental data and consider retraining models on relevant datasets for your specific application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
